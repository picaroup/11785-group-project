{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import csv\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "#import feature_extraction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/processed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clip_ID</th>\n",
       "      <th>Extraversion</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Speaker_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Status</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cut_feb1601-guest-0-2</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>-1.363636</td>\n",
       "      <td>1.545455</td>\n",
       "      <td>1.545455</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>92</td>\n",
       "      <td>M</td>\n",
       "      <td>J</td>\n",
       "      <td>[[0.00027620237, 0.0016822875, 0.001016233, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cut_feb1601-guest-2-10</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>1.909091</td>\n",
       "      <td>-1.727273</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>183</td>\n",
       "      <td>M</td>\n",
       "      <td>J</td>\n",
       "      <td>[[0.38929665, 0.1189508, 0.0048722113, 0.00183...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cut_feb1601-human-6</td>\n",
       "      <td>2.363636</td>\n",
       "      <td>-0.636364</td>\n",
       "      <td>1.363636</td>\n",
       "      <td>-0.545455</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>41</td>\n",
       "      <td>M</td>\n",
       "      <td>G</td>\n",
       "      <td>[[0.7792231, 0.29407483, 0.03487174, 0.0134306...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cut_feb1602-guest-1-5</td>\n",
       "      <td>1.272727</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>J</td>\n",
       "      <td>[[0.10963277, 0.09769615, 0.021039093, 0.02427...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cut_feb1602-guest-3-17</td>\n",
       "      <td>1.363636</td>\n",
       "      <td>-0.909091</td>\n",
       "      <td>2.181818</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>123</td>\n",
       "      <td>M</td>\n",
       "      <td>J</td>\n",
       "      <td>[[0.077622935, 0.14913887, 0.041750234, 0.0176...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Clip_ID  Extraversion  Agreeableness  Conscientiousness  \\\n",
       "0   cut_feb1601-guest-0-2      1.727273      -1.363636           1.545455   \n",
       "1  cut_feb1601-guest-2-10      0.727273       1.181818           1.909091   \n",
       "2     cut_feb1601-human-6      2.363636      -0.636364           1.363636   \n",
       "3   cut_feb1602-guest-1-5      1.272727       0.636364           1.818182   \n",
       "4  cut_feb1602-guest-3-17      1.363636      -0.909091           2.181818   \n",
       "\n",
       "   Neuroticism  Openness  Speaker_ID Gender Status  \\\n",
       "0     1.545455  0.727273          92      M      J   \n",
       "1    -1.727273  0.909091         183      M      J   \n",
       "2    -0.545455  0.363636          41      M      G   \n",
       "3    -0.454545  0.000000           3      M      J   \n",
       "4     0.909091  0.636364         123      M      J   \n",
       "\n",
       "                                            features  \n",
       "0  [[0.00027620237, 0.0016822875, 0.001016233, 0....  \n",
       "1  [[0.38929665, 0.1189508, 0.0048722113, 0.00183...  \n",
       "2  [[0.7792231, 0.29407483, 0.03487174, 0.0134306...  \n",
       "3  [[0.10963277, 0.09769615, 0.021039093, 0.02427...  \n",
       "4  [[0.077622935, 0.14913887, 0.041750234, 0.0176...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataframe into train, validation, and test sets\n",
    "train, val, test = np.split(data.sample(frac=1, random_state=101),\n",
    "[int(.7*len(data)), int(.85*len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['features'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 431)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['features'].iloc[5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriSamples(torch.utils.data.Dataset):\n",
    "    #Partition: the dataframe\n",
    "    def __init__(self, dataframe, test_order=None, trait=None):\n",
    "        #self.train=train\n",
    "\n",
    "        #The personality trait for this run through\n",
    "        self.trait = trait\n",
    "        self.dataframe = dataframe\n",
    "        #self.partition = partition\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe.index)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        #X = np.load(self.dataframe['features'].iloc[ind])\n",
    "        X = self.dataframe['features'].iloc[ind]\n",
    "        #Must transpose X so the the first value is length of audio and second value is number of features\n",
    "        X = X.transpose()\n",
    "        #if self.train:\n",
    "        #Y = np.array(self.dataframe[self.trait].iloc[ind])\n",
    "        Y = torch.tensor([self.dataframe[self.trait].iloc[ind]])\n",
    "        #if self.train:\n",
    "        return X, Y\n",
    "        #else:\n",
    "        #return X\n",
    "\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        batch_x = [torch.from_numpy(x) for x,y in batch]\n",
    "        batch_y = [y for x,y in batch]\n",
    "        batch_x_pad = pad_sequence(batch_x, batch_first=True)\n",
    "\n",
    "        lengths_x = [x.shape[0] for x in batch_x]\n",
    "\n",
    "        #The y's should only have a length of 1, but we'll pad it anyway\n",
    "\n",
    "        batch_y_pad = pad_sequence(batch_y, batch_first=True)\n",
    "        #lengths_y = [y.shape[0] for y in batch_y]\n",
    "        lengths_y = [y.shape[0] for y in batch_y]\n",
    "\n",
    "        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = LibriSamples(train, trait='Extraversion')\n",
    "val_data = LibriSamples(val, trait='Extraversion')\n",
    "test_data = LibriSamples(test, trait='Extraversion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 2e-3\n",
    "epochs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  64\n",
      "Train dataset samples = 448, batches = 7\n",
      "Val dataset samples = 96, batches = 2\n",
      "Test dataset samples = 96, batches = 2\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle=True, collate_fn = LibriSamples.collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size = batch_size, shuffle=False, collate_fn = LibriSamples.collate_fn) \n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle=False, collate_fn = LibriSamples.collate_fn) \n",
    "\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 431, 128]) torch.Size([64, 1]) torch.Size([64]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for data in val_loader:\n",
    "    x, y, lx, ly = data\n",
    "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self): # You can add any extra arguments as you wish\n",
    "\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=431, hidden_size = 256, num_layers=1, dropout=.1, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.classification = nn.Sequential(nn.ReLU(),\n",
    "                                            nn.Linear(512,1))\n",
    "            \n",
    "\n",
    "    def forward(self, x, x_length): \n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        packed_input = pack_padded_sequence(x,x_length, batch_first = True, enforce_sorted=False)\n",
    "\n",
    "        \n",
    "        #packed_input = pack_padded_sequence(x,length.clamp(max=max_expected_len), batch_first = True, enforce_sorted=False)\n",
    "        #pdb.set_trace()\n",
    "        out1, (out2, out3) =  self.lstm(packed_input) \n",
    "        # As you may see from the LSTM docs, LSTM returns 3 vectors. Which one do you need to pass to the next function?\n",
    "        out, lengths  = pad_packed_sequence(out1, batch_first=True) \n",
    "        #pdb.set_trace()\n",
    "        #pdb.set_trace()\n",
    "        out = self.classification(out) \n",
    "\n",
    "        #Is dimension 2 or 3? Or something else?\n",
    "   \n",
    "        #m = n.LogSoftmax(dim=2) # Optional: Do log softmax on the output. Which dimension?\n",
    "        out = F.log_softmax(out,dim=2)\n",
    "\n",
    "\n",
    "        return out, lengths \n",
    "\n",
    "#model = Network().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (lstm): LSTM(431, 256, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (classification): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cgwed\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Network().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cgwed\\Anaconda3\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "scaler = torch.cuda.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cgwed\\Anaconda3\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "start (8192) + length (64) exceeds dimension size (8192).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18944/1836458222.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mx_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18944/3317110980.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, x_length)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#pdb.set_trace()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#out1, (out2, out3) =  self.lstm(packed_input)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m# As you may see from the LSTM docs, LSTM returns 3 vectors. Which one do you need to pass to the next function?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    693\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[1;32m--> 695\u001b[1;33m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[0;32m    696\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: start (8192) + length (64) exceeds dimension size (8192)."
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = data[0].to(device)\n",
    "        y = data[1]\n",
    "        x_lengths = data[2]\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(x, x_lengths)\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "    total_loss += loss\n",
    "    batch_bar.set_postfix(\n",
    "        loss=\"{:.04f}\".format(float(total_loss/(i+1)))\n",
    "    )\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    batch_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1a47bf819ad7cd48b1d6f6457c70b719b72bc4eea580dc2d5ab4e404b26ab46"
  },
  "kernelspec": {
   "display_name": "Python (acram)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
